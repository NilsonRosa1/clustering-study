# -*- coding: utf-8 -*-
"""Tarefakmeans2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Ch8QIWRxE2KrQhtf6dIJEkNjJPGL9Kp

#**Importações:**

---



**Plotly.graph_objects:** *biblioteca do Plotly que permite criar visualizações de dados interativas e personalizadas, usando objetos gráficos como figuras, traçados, subplots e anotações.*

**Plotly.express:** *biblioteca de alto nível do Plotly que oferece uma interface simplificada para criar gráficos comuns como scatter plots, line plots, bar plots, histogramas, heatmaps e outros.*

**Numpy:** *biblioteca para computação numérica em Python, com funções para trabalhar com arrays multidimensionais, álgebra linear, operações matemáticas, randomização e outras operações numéricas.*

**Sklearn.preprocessing (StandardScaler):** *classe do Scikit-learn que permite pré-processar dados numéricos para ajustá-los à escala e distribuição adequadas para um modelo de aprendizado de máquina. A classe StandardScaler realiza a padronização dos dados, subtraindo a média e dividindo pelo desvio padrão.*

**Pandas:** *biblioteca para análise de dados em Python, com funções para trabalhar com estruturas de dados como DataFrames e Series, além de manipulação, agregação, seleção, filtragem e transformação de dados.*

**Sklearn.cluster (KMeans):** *classe do Scikit-learn para clustering, um método de aprendizado de máquina não supervisionado que agrupa dados em clusters com base na similaridade entre eles. A classe KMeans implementa o algoritmo de clustering K-Means, que divide o conjunto de dados em K clusters, minimizando a soma das distâncias quadráticas entre cada ponto e o centróide do seu respectivo cluster.* 

**Seaborn:** *É uma biblioteca de visualização de dados em Python baseada no Matplotlib, mas com uma sintaxe mais simplificada e mais atrativa visualmente. Ele oferece diversas funções para criar gráficos estatísticos, como histogramas, gráficos de densidade, scatter plots, heatmaps, entre outros, além de possuir recursos avançados para visualização de distribuições, relações entre variáveis e ajuste de modelos. Com o Seaborn, é possível criar visualizações de dados atraentes e informativas de forma rápida e eficiente.*
"""

import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.cluster import KMeans
import seaborn as sns

"""#*Extração dos Dados*


*Este projeto utiliza uma base de dados fornecida pelo professor para fins de análise e clusterização.
Para acessar os dados, utilizamos as bibliotecas pandas, que nos permitem ler e manipular arquivos de dados com facilidade. As informações são armazenadas em um dataframe, que é uma estrutura de dados tabular semelhante a uma planilha, composta por linhas e colunas.*
"""

df = pd.read_csv('/content/credit_card_clients.csv')

"""#**Análise Exploratória dos Dados (E.D.A)**

*Nessa etapa, vamos realizar uma Análise Exploratória dos Dados (EDA) para entender melhor a base de dados fornecida pelo professor. É importante ressaltar que, para aplicar técnicas de clusterização, é fundamental entendermos a natureza dos dados e identificarmos possíveis problemas e oportunidades de melhoria.*

*A EDA pode parecer simples em um primeiro contato, mas é uma etapa crucial para garantir que estamos utilizando a técnica mais adequada para o nosso caso específico. Cada base de dados possui suas particularidades e desafios únicos, e a EDA nos ajuda a identificar as melhores estratégias para lidar com essas questões.*

*Durante a EDA, podemos explorar a distribuição dos dados, identificar a presença de outliers, analisar a correlação entre as variáveis, entre outras técnicas. Isso nos permite ter uma compreensão mais profunda da base de dados e tomar decisões mais informadas na etapa de pré-processamento.*

*Em resumo, a Análise Exploratória dos Dados é uma etapa fundamental para o sucesso de qualquer projeto de clusterização. Por meio dela, podemos entender melhor os dados que estamos trabalhando e selecionar as melhores técnicas para extrair informações úteis a partir da base de dados.*
"""

df.columns

df.describe()

df.shape

df.info()

df.head(5)

df= df.iloc[1:] # percebemos que  tinha mais uma features  sendo utilizada de amostra, então tiramos

'''A abordagem será analisar Limit_bal com a educação'''

sns.histplot(df['X1']) # histograma da variável

sns.kdeplot(df['X1'].astype(int)) # plot da densidade da variável

sns.histplot(df['X3']) # histograma da variável

sns.kdeplot(df['X3'].astype(int)) # plot da densidade da variável

corr = df['X1'].astype(int).corr(df['X3'].astype(int))
print('Coeficiente de correlação:', corr)
print('Isso significa que, quando o valor de uma variável aumenta, o valor da outra variável tende a diminuir, mas essa relação não é forte.')

'''O gráfico gerado mostrará os boxplots para cada coluna,
 onde qualquer valor que esteja fora do limite superior 
 ou inferior do boxplot é considerado um outlier.'''

sns.boxplot(data=df[['X1', 'X3']].astype(int))

x = df['X1'].astype(int)
y = df['X3'].astype(int)

grafico = px.scatter(x = x, y = y)
grafico.show()

"""#**pré-processamento de dados** 

*É uma etapa essencial em qualquer projeto de mineração de dados, incluindo a clusterização. O objetivo do pré-processamento é preparar os dados brutos para serem utilizados em algoritmos de aprendizado de máquina, removendo informações redundantes, corrigindo dados faltantes, normalizando e padronizando os dados.*

***Algumas técnicas comuns de pré-processamento de dados utilizadas em clusterização incluem:***

**Tratamento de valores faltantes:** quando há valores faltantes nos dados, eles podem ser preenchidos com uma estimativa baseada em outras amostras ou eliminados completamente*.

**Detecção e remoção de outliers:** *outliers podem afetar significativamente a clusterização, por isso é importante detectá-los e removê-los para evitar que distorçam os resultados.*

**Normalização e padronização dos dados:** *técnicas como a normalização z-score e min-max scaling podem ser aplicadas para colocar os dados em uma escala comum e evitar que características com valores maiores dominem a clusterização.*

**Redução de dimensionalidade:** *quando há muitas variáveis ou características nos dados, a clusterização pode ser afetada pela maldição da dimensionalidade. Técnicas como a Análise de Componentes Principais (PCA) podem ser usadas para reduzir a dimensionalidade dos dados.*

**Seleção de características:** *selecionar apenas as características mais importantes dos dados pode melhorar a qualidade da clusterização e reduzir o tempo de processamento.*

Ao realizar um pré-processamento adequado, é possível melhorar a qualidade da clusterização e obter insights mais precisos e úteis a partir dos dados.:
"""

#Tratamento de valores faltantes: sem dados faltantes

print('Total de Amostra de x:',x.count())

num_nulls = x.isnull().sum()
print('Total de Dados Faltantes de x:',num_nulls)

print('Total de Amostra de y:',y.count())

num_nulls = x.isnull().sum()

print('Total de Dados Faltantes de y:',num_nulls)

x.astype(float)
y.astype(float)

x = pd.to_numeric(x, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

print('valores nullos em x:',x.isna().sum())
print('valores nullos em y:',y.isna().sum())

#Detecção e remoção de outliers: 


'''Este é apenas um exemplo simples de detecção e remoção de outliers em uma série usando a regra dos 3 sigmas.'''


# criando uma máscara booleana para detectar outliers em x e y
mean_x = x.mean()
std_x = x.std()
lower_limit_x = mean_x - 3*std_x
upper_limit_x = mean_x + 3*std_x
mask_x = (x < lower_limit_x) | (x > upper_limit_x)

mean_y = y.mean()
std_y = y.std()
lower_limit_y = mean_y - 3*std_y
upper_limit_y = mean_y + 3*std_y
mask_y = (y < lower_limit_y) | (y > upper_limit_y)

# removendo os outliers de x e y simultaneamente
mask = np.logical_not(mask_x | mask_y)
x= x[mask]
y = y[mask]

# agora x_clean e y_clean tem os mesmos valores e na mesma ordem

# Normalização e padronização dos dados:

'''O .T no final transpõe a matriz, 
de forma que a primeira coluna do DataFrame se torna a primeira coluna do array
e a segunda coluna do DataFrame se torna a segunda coluna do array.'''

base_salario = np.array([x.values, y.values]).T

base_salario

"""Visualizando se as ordem dos index foram afetadas pela remoção dos outliers e ruídos



``` 
#df.head(-1)
```



```
# df = pd.concat([x, y], axis=1)

df.head(-1)
```

#Modelagem 
*Na clusterização envolve a escolha de um algoritmo de clusterização, definição de um número de clusters adequado, seleção de recursos relevantes e, em seguida, treinamento do modelo e avaliação de sua eficácia.*


*K-means é um algoritmo de clustering amplamente utilizado na modelagem de dados para identificar grupos ou clusters de objetos semelhantes.*
"""

scaler_salario = StandardScaler()
base_salario = scaler_salario.fit_transform(base_salario)

kmeans_salario = KMeans(n_clusters=3)
kmeans_salario.fit(base_salario)

centroides = kmeans_salario.cluster_centers_
centroides

scaler_salario.inverse_transform(kmeans_salario.cluster_centers_)

rotulos = kmeans_salario.labels_
rotulos

grafico1 = px.scatter(x = base_salario[:,0], y = base_salario[:, 1], color = rotulos)
grafico2 = px.scatter(x = centroides[:,0], y = centroides[:, 1], size = [12,12,12])
grafico3 = go.Figure(data = grafico1.data + grafico2.data)
grafico3.show()

df = pd.concat([x, y], axis=1)

df['cluster'] = rotulos

df['cluster'].value_counts()

df['cluster'] = df['cluster'].replace({0: 'A', 1: 'B', 2: 'C'})

df = df.rename(columns={'cluster': 'Rotulos'})

df.head(10)